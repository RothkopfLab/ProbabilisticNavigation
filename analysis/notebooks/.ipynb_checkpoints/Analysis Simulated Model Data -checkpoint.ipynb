{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "050c0bf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fabiankessler/.pyenv/versions/3.9.9/envs/statistics/lib/python3.9/site-packages/outdated/utils.py:14: OutdatedPackageWarning: The package outdated is out of date. Your version is 0.2.1, the latest is 0.2.2.\n",
      "Set the environment variable OUTDATED_IGNORE=1 to disable these warnings.\n",
      "  return warn(\n",
      "/Users/fabiankessler/.pyenv/versions/3.9.9/envs/statistics/lib/python3.9/site-packages/outdated/utils.py:14: OutdatedPackageWarning: The package pingouin is out of date. Your version is 0.5.1, the latest is 0.5.3.\n",
      "Set the environment variable OUTDATED_IGNORE=1 to disable these warnings.\n",
      "  return warn(\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import copy\n",
    "import time\n",
    "import pickle\n",
    "import random\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "from numpy import matlib as mb\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import seaborn as sns \n",
    "import pingouin as pg \n",
    "\n",
    "\n",
    "#set plotting \n",
    "%matplotlib notebook\n",
    "\n",
    "colours = {'self-motion' : (0.8545098039215687, 0.1415686274509803, 0.14862745098039193),\n",
    "           'landmark' : (0.24098039215686273, 0.49156862745098046, 0.6962745098039216),\n",
    "           'combined' : (0.32058823529411773, 0.6664705882352941, 0.31000000000000005),\n",
    "           'conflict' : (0.5837254901960787, 0.3225490196078431, 0.6225490196078431), \n",
    "           'ground' : (0.5837254901960787, 0.3225490196078431, 0.6225490196078431)}\n",
    "\n",
    "colours['cue-integration'] = 'grey'\n",
    "colours['a-integration'] = 'grey'\n",
    "trial_types = {1: 'landmark', 2: 'self-motion', 3: 'combined', 4: 'conflict'}\n",
    "order = {'self-motion' : 0, 'landmark' : 1, 'combined' : 2, 'conflict' : 3, 'cue-integration' : 4}\n",
    "\n",
    "from IPython.display import SVG, display\n",
    "def show_svg(file):\n",
    "    display(SVG(filename=file))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d4e917",
   "metadata": {},
   "source": [
    "# Nardini 2008 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e90a6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Nardini 2008\n",
    "%matplotlib inline\n",
    "from IPython.display import SVG, display\n",
    "def show_svg(file):\n",
    "    display(SVG(filename=file))\n",
    "\n",
    "path = '../temp_figures/'\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "#import functions \n",
    "from analysis.nardini2008.utils import * \n",
    "from analysis.nardini2008.preprocessing import preprocess_nardini_2008, preprocress_per_participant\n",
    "from analysis.nardini2008.cue_integration_model import cue_integration_nardini2008_model\n",
    "from analysis.nardini2008.pipeline import plot_all_conditions, extract_SDs\n",
    "\n",
    "from analysis.key_plots import plot_endpoint_variability, plot_raw_endpoints, annotate_plot\n",
    "from analysis.endpoint_comparison import * \n",
    "\n",
    "################################################################################################\n",
    "                                    ### Endpoint Analysis ###\n",
    "################################################################################################\n",
    "\n",
    "# _____________________________________ Data Normalized ________________________________________ #\n",
    "dataset_simulated = pd.read_csv('../data/nardini2008_endpoint_data.csv')\n",
    "normalized_data_simulated, SDs_simulated = preprocess_nardini_2008(dataset_simulated, \n",
    "                                                                   remove_outliers=True, \n",
    "                                                                   normalize=True, \n",
    "                                                                   flip_conclict=True)\n",
    "\n",
    "# generate nardini plot \n",
    "output = plot_all_conditions(path, normalized_data_simulated)\n",
    "output.save(path + 'nardini_model_simulation_colours.svg')\n",
    "show_svg(path + 'nardini_model_simulation_colours.svg')\n",
    "\n",
    "# _____________________________________ Cue Integration Analysis ________________________________________ #\n",
    "\n",
    "temp_dfs = [] \n",
    "for VP, data in normalized_data_simulated.groupby('VPCode'): \n",
    "    SDs, results = cue_integration_nardini2008_model(data)\n",
    "    \n",
    "    #includes the prediction\n",
    "    prediction = {'VPCode' : SDs.VPCode.iloc[0], 'condition' : 'cue-integration', 'x' : 0, 'y' : 0, 'SD' : np.sqrt(results['sigmas']['cue-integration']), 'kind' : 'Model', 'order' : order['cue-integration']}\n",
    "    SDs = pd.concat([SDs, pd.DataFrame(prediction, index = [0])])\n",
    "    temp_dfs.append(SDs)\n",
    "\n",
    "    \n",
    "SDs = pd.concat(temp_dfs)\n",
    "SDs = SDs.reset_index()\n",
    "SDs_simulated = SDs.sort_values('order', ascending=True)\n",
    "SDs_simulated['kind'] = 'Model'\n",
    "\n",
    "SDs_full = SDs_simulated\n",
    "SDs_full['VPCode'] = SDs_full['VPCode'].astype(str)\n",
    "\n",
    "\n",
    "hue_order = ['self-motion', 'landmark', 'combined', 'conflict', 'cue-integration']\n",
    "\n",
    "### Plot Functions \n",
    "path = '../../../figures/'\n",
    "plt.figure(figsize = (3.75,3.75/1.5))    \n",
    "p = sns.barplot(x = 'kind', y = 'SD', hue = 'condition', hue_order=hue_order ,data = SDs_full, palette=colours)\n",
    "plt.suptitle('Response Variability (Nardini 2008)')\n",
    "plt.ylabel('Response Variability \\n (Euclidean Distance SD,m)')\n",
    "plt.xlabel('')\n",
    "plt.ylim(0,1)\n",
    "#plt.yticks(np.arange(0,1.1,0.1))\n",
    "#p.legend(fontsize=10)\n",
    "p.legend_.remove()\n",
    "sns.despine()\n",
    "\n",
    "# _____________________________________ Predictions & Conflict Conditions ________________________________________ #\n",
    "\n",
    "x = np.linspace(0,1,100)\n",
    "\n",
    "paired_colors = sns.color_palette(\"Paired\", 2)\n",
    "\n",
    "def compute_d_nardini_formula(mean_locations, rotated_target_location = np.array([ 0.45293333 -0.0596298 ])):\n",
    "    d_lm = np.linalg.norm(rotated_target_location -  mean_locations['conflict'])\n",
    "    d_sm = np.linalg.norm(np.zeros(2) -  mean_locations['conflict'])\n",
    "                              \n",
    "    return d_lm, d_sm\n",
    "\n",
    "#equation 1\n",
    "def relative_proximity(d_lm, d_sm): \n",
    "    rprox_lm = d_sm / (d_sm + d_lm)\n",
    "    rprox_sm = 1 - rprox_lm\n",
    "    return rprox_lm, rprox_sm\n",
    "\n",
    "def get_combined_variance(w, sm, lm):\n",
    "    return (1-w)**2 * sm + w**2 * lm\n",
    "\n",
    "\n",
    "def calculate_cue_weights(sigma_lm, sigma_sm): \n",
    "    w_lm = sigma_sm / (sigma_lm + sigma_sm)\n",
    "    w_sm = 1 - w_lm \n",
    "    return w_lm, w_sm  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "x = np.linspace(0,1,1000)\n",
    "\n",
    "\n",
    "\n",
    "plt.figure(figsize = (3.25, 3.25 / 1.75))\n",
    "\n",
    "VPs_rprox = {}\n",
    "VPs_cue_weight = {}\n",
    "\n",
    "for VP, vp_data in SDs_full.groupby('VPCode'):\n",
    "    resp_means = {}\n",
    "    \n",
    "    for cond, cond_data in vp_data.groupby('condition'):\n",
    "        resp_means[cond] = cond_data[['x','y']].values\n",
    "        \n",
    "    d_lm, d_sm = compute_d_nardini_formula(resp_means)\n",
    "    sigma_sm = vp_data[vp_data.condition == 'self-motion']['SD'].values**2 \n",
    "    sigma_lm = vp_data[vp_data.condition == 'landmark']['SD'].values**2 \n",
    "    \n",
    "    rprox_lm, rprox_sm = relative_proximity(d_lm, d_sm)\n",
    "    w_lm, w_sm = calculate_cue_weights(sigma_lm, sigma_sm)\n",
    "    \n",
    "    VPs_rprox[VP] = rprox_lm\n",
    "    VPs_cue_weight[VP] = w_lm\n",
    "    \n",
    "    \n",
    "SDs_full['rp'] = SDs_full.apply(lambda x : VPs_rprox[x['VPCode']], axis = 1)\n",
    "SDs_full['w_lm'] = SDs_full.apply(lambda x : VPs_cue_weight[x['VPCode']], axis = 1)\n",
    "\n",
    "\n",
    "SDs = (SDs_full.groupby(['condition', 'kind']).mean()).to_dict()['SD']\n",
    "sm_variance =  SDs[('self-motion', 'Model')]**2\n",
    "lm_variance =  SDs[('landmark', 'Model')]**2\n",
    "combined_variance = SDs[('combined', 'Model')]**2\n",
    "conflict_variance = SDs[('conflict', 'Model')]**2\n",
    "y_model_mean = np.sqrt(get_combined_variance(x, sm_variance, lm_variance))\n",
    "\n",
    "SDs = (SDs_full.groupby(['condition', 'kind']).mean() + SDs_full.groupby(['condition', 'kind']).std()).to_dict()['SD']\n",
    "sm_variance =  SDs[('self-motion', 'Model')]**2\n",
    "lm_variance =  SDs[('landmark', 'Model')]**2\n",
    "combined_variance = SDs[('combined', 'Model')]**2\n",
    "conflict_variance = SDs[('conflict', 'Model')]**2\n",
    "y_model_upper = np.sqrt(get_combined_variance(x, sm_variance, lm_variance))\n",
    "\n",
    "\n",
    "SDs = (SDs_full.groupby(['condition', 'kind']).mean() - SDs_full.groupby(['condition', 'kind']).std()).to_dict()['SD']\n",
    "sm_variance =  SDs[('self-motion', 'Model')]**2\n",
    "lm_variance =  SDs[('landmark', 'Model')]**2\n",
    "combined_variance = SDs[('combined', 'Model')]**2\n",
    "conflict_variance = SDs[('conflict', 'Model')]**2\n",
    "y_model_lower = np.sqrt(get_combined_variance(x, sm_variance, lm_variance))\n",
    "\n",
    "\n",
    "plt.plot(x, y_model_mean, color = paired_colors[0])\n",
    "plt.fill_between(x, y_model_lower, y_model_upper, alpha = 0.25, color = paired_colors[0])\n",
    "\n",
    "\n",
    "plt.axvline(x[np.argmin(y_model_mean)], label = 'optimal', color = paired_colors[0], ls = '--')\n",
    "\n",
    "combined_sds = SDs_full[SDs_full.condition == 'conflict']\n",
    "\n",
    "combined_sds_model = combined_sds[combined_sds.kind == 'Model']\n",
    "\n",
    "\n",
    "plt.suptitle('Predictions vs. Behavior \\n in the Conflict Condition', y = 1.1)\n",
    "plt.xlim(0,1)\n",
    "plt.ylim(0,1)\n",
    "\n",
    "plt.xticks(np.arange(0,1.25,0.25))\n",
    "plt.yticks(np.arange(0,1.1,0.25))\n",
    "\n",
    "plt.xlabel('Relative Landmark Proximity \\n (measured)')\n",
    "plt.ylabel('Response Variability \\n SD (m) (predicted)')\n",
    "\n",
    "plt.legend()\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c862ac07",
   "metadata": {},
   "source": [
    "# Chen 2017 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3df871b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Chen 2017 \n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "#path \n",
    "path = '../temp_figures/'\n",
    "\n",
    "#import functions \n",
    "from analysis.chen2017.data_loading import load_chen_data, process_simulated_data\n",
    "from analysis.chen2017.utils import * \n",
    "from analysis.chen2017.preprocessing import preprocess_data_chen_2017, preprocess_per_participant\n",
    "from analysis.chen2017.cue_integration_model import chen_2017_cue_integration_model, compute_cue_weights, compute_sds_per_vp\n",
    "from analysis.chen2017.pipeline import * \n",
    "\n",
    "from analysis.key_plots import *\n",
    "from analysis.endpoint_comparison import * \n",
    "\n",
    "from IPython.display import SVG, display\n",
    "def show_svg(file):\n",
    "    display(SVG(filename=file))\n",
    "\n",
    "################################################################################################\n",
    "                                    ### Endpoint Analysis ###\n",
    "################################################################################################\n",
    "poor_dataset_simulated = pd.read_csv('../data/chen2017_poor_endoint_data.csv')\n",
    "rich_dataset_simulated = pd.read_csv('../data/chen2017_rich_endoint_data.csv')\n",
    "\n",
    "# _____________________________________ Data Normalized ________________________________________ #\n",
    "\n",
    "#normalize data via chen preprocessing pipeline \n",
    "\n",
    "#add the target jitter to the response coordinates prior to normalization\n",
    "rich_dataset_simulated[['targetx', 'targetz']] += rich_dataset_simulated[['jitterx', 'jitterz']].values\n",
    "poor_dataset_simulated[['targetx', 'targetz']] += poor_dataset_simulated[['jitterx', 'jitterz']].values\n",
    "\n",
    "\n",
    "#rich \n",
    "rich_normalized_data_simulated = preprocess_data_chen_2017(rich_dataset_simulated, recenter = False, remove_outliers=True, iqr = 3)\n",
    "per_condition_plot = generate_per_condition_plot(rich_normalized_data_simulated, path = path)\n",
    "show_svg(per_condition_plot)\n",
    "\n",
    "#poor \n",
    "poor_normalized_data_simulated = preprocess_data_chen_2017(poor_dataset_simulated, recenter = False, remove_outliers=True, iqr = 3)\n",
    "per_condition_plot = generate_per_condition_plot(poor_normalized_data_simulated, path = path)\n",
    "show_svg(per_condition_plot)\n",
    "\n",
    "# _____________________________________ Cue Integration Analysis ________________________________________ #\n",
    "\n",
    "#simulated \n",
    "rich_sds_simulated = compute_sds_per_vp(rich_dataset_simulated, environment = 'rich', kind = 'Model')\n",
    "poor_sds_simulated = compute_sds_per_vp(poor_dataset_simulated, environment = 'poor', kind = 'Model')\n",
    "\n",
    "rich_sds_simulated['SD'] = rich_sds_simulated['value']\n",
    "poor_sds_simulated['SD'] = poor_sds_simulated['value']\n",
    "\n",
    "\n",
    "rich_sds_simulated['environment'] = '3 lm'\n",
    "poor_sds_simulated['environment'] = '1 lm'\n",
    "\n",
    "\n",
    "# Plotting \n",
    "figsize = (3.25,2)\n",
    "\n",
    "import seaborn as sns \n",
    "plt.figure(figsize = figsize)    \n",
    "p = sns.barplot(x = 'environment', y = 'value', hue = 'condition', data = pd.concat([rich_sds_simulated, poor_sds_simulated]), palette = colours)\n",
    "plt.suptitle('Response Variability')\n",
    "plt.ylabel('sd (m)')\n",
    "plt.xlabel('')\n",
    "#plt.ylim(0,1.5)\n",
    "plt.yticks(np.arange(0,1.1,0.1))\n",
    "p.legend(fontsize=10)\n",
    "p.legend_.remove()\n",
    "sns.despine()\n",
    "plt.suptitle('Chen 2017 \\n 3 vs. 1 Landmark', y = 1.1)\n",
    "plt.ylabel('Response Variability \\n (Euclidean Distance SD,m)')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "################################################################################################\n",
    "                                    ### Additional Analysis ###\n",
    "################################################################################################\n",
    "\n",
    "# _____________________________________ Cue Weight Regression Analysis ________________________________________ #\n",
    "\n",
    "import statsmodels.api as sm\n",
    "\n",
    "\n",
    "def fit_OLS(predicted, empirical):\n",
    "    x = predicted\n",
    "    y = empirical\n",
    "    X = sm.add_constant(predicted)\n",
    "\n",
    "    model = sm.OLS(y, X)\n",
    "    results = model.fit()\n",
    "\n",
    "    return model, results\n",
    "\n",
    "\n",
    "#Simulated (Model)\n",
    "VPs_rich, predicted_rich, empirical_rich = compute_cue_weights(rich_dataset_simulated)\n",
    "VPs_poor, predicted_poor, empirical_poor = compute_cue_weights(poor_dataset_simulated)\n",
    "\n",
    "\n",
    "plt.figure(figsize = (3.25,2.25))\n",
    "\n",
    "plt.scatter(predicted_rich, empirical_rich, label = '3 lm')\n",
    "plt.scatter(predicted_poor, empirical_poor, label = '1 lm')\n",
    "\n",
    "model, results_simulated_rich = fit_OLS(predicted_rich, empirical_rich)\n",
    "xfit = np.linspace(np.min(predicted_rich), np.max(predicted_rich), 1000)\n",
    "X = sm.add_constant(xfit)\n",
    "yfit = results_simulated_rich.predict(X)\n",
    "plt.plot(xfit, yfit);\n",
    "\n",
    "model, results_simulated_poor = fit_OLS(predicted_poor, empirical_poor)\n",
    "xfit = np.linspace(np.min(predicted_poor), np.max(predicted_poor), 1000)\n",
    "X = sm.add_constant(xfit)\n",
    "yfit = results_simulated_poor.predict(X)\n",
    "plt.plot(xfit, yfit);\n",
    "\n",
    "#combined\n",
    "model, results_simulated_combined = fit_OLS(predicted_rich + predicted_poor, empirical_rich + empirical_poor)\n",
    "xfit = np.linspace(np.min(predicted_rich), np.max(predicted_rich), 1000)\n",
    "X = sm.add_constant(xfit)\n",
    "yfit = results_simulated_combined.predict(X)\n",
    "#plt.plot(xfit, yfit);\n",
    "\n",
    "\n",
    "plt.plot((0,1), (0,1), ls = '--', color = 'black')\n",
    "plt.xlim(0,1)\n",
    "plt.ylim(0,1)\n",
    "plt.xlabel('optimal cue weight')\n",
    "plt.ylabel('')\n",
    "plt.suptitle('Model')\n",
    "#print(model.coef_)\n",
    "#print(model.intercept_)\n",
    "frame1 = plt.gca()\n",
    "#frame1.axes.xaxis.set_ticklabels([])\n",
    "frame1.axes.yaxis.set_ticklabels([])\n",
    "\n",
    "sns.despine()\n",
    "plt.legend(frameon = False, bbox_to_anchor=(0.25, -0.25), title = 'environment', ncol = 2)\n",
    "\n",
    "\n",
    "# _____________________________________ Response Mean Analysis ________________________________________ #\n",
    "\n",
    "fig, ax = plt.subplots(2,2, figsize = (2.5,2.5), sharex=True, sharey=True)\n",
    "use_response_bias_cleared_means = False\n",
    "\n",
    "ax[0,0].set_xlim(-1,1)\n",
    "ax[0,0].set_ylim(-1,1)\n",
    "ax[0,0].set_xticks(np.arange(-1,1.5, 1))\n",
    "ax[0,0].set_yticks(np.arange(-1,1.5, 1))\n",
    "ax[0,0].set_ylim(-1,1)\n",
    "\n",
    "ax[0,0].set_title('Chen 2017')\n",
    "ax[0,1].set_title('Model')\n",
    "\n",
    "conflict_data = rich_normalized_data_simulated[rich_normalized_data_simulated.condition == 'conflict']\n",
    "landmark_data = rich_normalized_data_simulated[rich_normalized_data_simulated.condition == 'landmark']\n",
    "conflict_means = conflict_data.groupby('VPCode').mean()[['x', 'y']].values\n",
    "cleared_means = (conflict_data.groupby('VPCode').mean()[['x', 'y']] - landmark_data.groupby('VPCode').mean()[['x', 'y']]).values\n",
    "\n",
    "ax[0,1].scatter(0,0, marker = '+', s = 200, color = colours['self-motion'])\n",
    "ax[0,1].scatter(0.55774169,-0.073428087, marker = '+', s = 200, color = colours['landmark'])\n",
    "\n",
    "if use_response_bias_cleared_means:\n",
    "    ax[0,1].scatter(cleared_means[:,0], cleared_means[:,1], color = colours['conflict'], alpha = 0.25)\n",
    "else:\n",
    "    ax[0,1].scatter(conflict_means[:,0], conflict_means[:,1], color = colours['conflict'], alpha = 0.25)\n",
    "\n",
    "conflict_data = poor_normalized_data_simulated[poor_normalized_data_simulated.condition == 'conflict']\n",
    "landmark_data = poor_normalized_data_simulated[poor_normalized_data_simulated.condition == 'landmark']\n",
    "conflict_means = conflict_data.groupby('VPCode').mean()[['x', 'y']].values\n",
    "cleared_means = (conflict_data.groupby('VPCode').mean()[['x', 'y']] - landmark_data.groupby('VPCode').mean()[['x', 'y']]).values\n",
    "\n",
    "ax[1,1].scatter(0,0, marker = '+', s = 200, color = colours['self-motion']) #self-motion prediction\n",
    "ax[1,1].scatter(0.55774169,-0.073428087, marker = '+', s = 200, color = colours['landmark']) #landmark prediction \n",
    "\n",
    "if use_response_bias_cleared_means:\n",
    "    ax[1,1].scatter(cleared_means[:,0], cleared_means[:,1], color = colours['conflict'], alpha = 0.25)\n",
    "else:\n",
    "    ax[1,1].scatter(conflict_means[:,0], conflict_means[:,1], color = colours['conflict'], alpha = 0.25)\n",
    "\n",
    "    \n",
    "#ax[1,1].set_title('poor')\n",
    "\n",
    "ax[1,0].set_aspect('equal')\n",
    "ax[0,1].set_aspect('equal')\n",
    "ax[1,1].set_aspect('equal')\n",
    "ax[0,0].set_aspect('equal')\n",
    "\n",
    "\n",
    "ax[0,0].set_ylabel('y (m)')\n",
    "ax[1,0].set_ylabel('y (m)')\n",
    "ax[1,0].set_xlabel('x (m)')\n",
    "ax[1,1].set_xlabel('x (m)')\n",
    "\n",
    "#plt.suptitle('Subject Mean Responses \\n in Conflict Conditions')\n",
    "\n",
    "lines_labels = [ax.get_legend_handles_labels() for ax in fig.axes]\n",
    "lines, labels = [sum(lol, []) for lol in zip(*lines_labels)]\n",
    "fig.legend(lines, labels, loc = 'lower center', bbox_to_anchor=(0.415, -0.265, 0.1, 0.4), ncol = 2, title = 'consistent location', frameon = False)\n",
    "\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b4a779",
   "metadata": {},
   "source": [
    "# Zhao 2015 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "599cb805",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'evaluate_zhao_cue_integration' from 'analysis.zhao2015.cue_integration_model' (/Users/fabiankessler/Dropbox/PhD/00-Projects/ProbabilisticNavigation/submission/ncomm-sequential-uncertainties/analysis/notebooks/../analysis/zhao2015/cue_integration_model.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01manalysis\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mzhao2015\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m \n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01manalysis\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mzhao2015\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m fit_von_mises, fit_von_mises_per_condition_df, normalize_data_by_conflict_condition\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01manalysis\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mzhao2015\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcue_integration_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m zhao_2015a_cue_integration_model, evaluate_zhao_cue_integration\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01manalysis\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mzhao2015\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpipeline\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01manalysis\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkey_plots\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'evaluate_zhao_cue_integration' from 'analysis.zhao2015.cue_integration_model' (/Users/fabiankessler/Dropbox/PhD/00-Projects/ProbabilisticNavigation/submission/ncomm-sequential-uncertainties/analysis/notebooks/../analysis/zhao2015/cue_integration_model.py)"
     ]
    }
   ],
   "source": [
    "# Zhao 2015 \n",
    "%matplotlib inline\n",
    "\n",
    "#path \n",
    "path = '../temp_figures/'\n",
    "\n",
    "annotate = True\n",
    "\n",
    "#import functions \n",
    "from analysis.zhao2015.data_loading import load_zhao_data,load_zhao_data_simulated\n",
    "from analysis.zhao2015.utils import * \n",
    "from analysis.zhao2015.preprocessing import fit_von_mises, fit_von_mises_per_condition_df, normalize_data_by_conflict_condition\n",
    "from analysis.zhao2015.cue_integration_model import zhao_2015a_cue_integration_model, evaluate_zhao_cue_integration\n",
    "from analysis.zhao2015.pipeline import *\n",
    "\n",
    "from analysis.key_plots import *\n",
    "from analysis.endpoint_comparison import * \n",
    "\n",
    "import seaborn as sns\n",
    "from IPython.display import SVG, display\n",
    "def show_svg(file):\n",
    "    display(SVG(filename=file))\n",
    "\n",
    "\n",
    "################################################################################################\n",
    "                                    ### Endpoint Analysis ###\n",
    "################################################################################################\n",
    "\n",
    "# _____________________________________ Data Loading ________________________________________ #\n",
    "\n",
    "print('data loading')\n",
    "proximal_dataset_simulated = pd.read_csv('../data/zhao2015_proximal_endpoint_data.csv')\n",
    "distal_dataset_simulated = pd.read_csv('../data/zhao2015_distal_endpoint_data.csv')\n",
    "\n",
    "# _____________________________________  Normalize Data ________________________________________ #\n",
    "\n",
    "proximal_normalized_data_simulated = normalize_data_simulated(proximal_dataset_simulated)\n",
    "distal_normalized_data_simulated = normalize_data_simulated(distal_dataset_simulated)\n",
    "\n",
    "# remove heading outliers \n",
    "proximal_normalized_data_simulated = remove_outliers_heading_direction(proximal_normalized_data_simulated) \n",
    "distal_normalized_data_simulated = remove_outliers_heading_direction(distal_normalized_data_simulated) \n",
    "proximal_normalized_data_simulated['condition'] = proximal_normalized_data_simulated.condition.apply(lambda x : x.split('_')[0])\n",
    "distal_normalized_data_simulated['condition'] = distal_normalized_data_simulated.condition.apply(lambda x : x.split('_')[0])\n",
    "\n",
    "# _____________________________________ Plot Data Normalized ________________________________________ #\n",
    "\n",
    "all_plot, conflict_plot = generate_endpoint_plot(proximal_normalized_data_simulated, 'proximal', path, limit = 5.5, center = False, flip = True)\n",
    "show_svg(all_plot)\n",
    "show_svg(conflict_plot)\n",
    "\n",
    "all_plot, conflict_plot = generate_endpoint_plot(distal_normalized_data_simulated, 'distal', path, limit = 5.5, center = False, flip = True)\n",
    "show_svg(all_plot)\n",
    "show_svg(conflict_plot)\n",
    "\n",
    "print('cue integration analysis (circular)')\n",
    "\n",
    "\n",
    "#remove extreme conflict conditions \n",
    "proximal_dataset_simulated = proximal_dataset_simulated[proximal_dataset_simulated.landmark_rotation < 100]\n",
    "distal_dataset_simulated = distal_dataset_simulated[distal_dataset_simulated.landmark_rotation < 100]\n",
    "\n",
    "#Computing \n",
    "VP_cue_integration_simulated = {}\n",
    "cue_integration_results = []\n",
    "\n",
    "recenter = True\n",
    "\n",
    "for experiment, dataset_simulated in zip(['proximal', 'distal'], [proximal_dataset_simulated, distal_dataset_simulated]):\n",
    "    print('    ' + experiment)\n",
    "    \n",
    "    VPs_simulated = []\n",
    "    \n",
    "    for VP, data in dataset_simulated.groupby('VPCode'):\n",
    "        print(VP)\n",
    "        #data = remove_per_side_bias(data)\n",
    "        full_data = data \n",
    "        \n",
    "        if recenter:\n",
    "            condition_data = copy.deepcopy(data[data['condition'] != 'conflict'])\n",
    "            condition_data['condition'] = condition_data['condition'] + '_' + condition_data['target'].astype(str)\n",
    "\n",
    "            conflict_data = copy.deepcopy(data[data['condition'] == 'conflict'])\n",
    "            conflict_data['condition'] = conflict_data['condition'] + '_' + conflict_data['target'].astype(str) + '_' + conflict_data['landmark_rotation'].astype(str) + '_' + conflict_data['lmturn'].astype(str)\n",
    "\n",
    "            full_data = pd.concat([condition_data, conflict_data])\n",
    "        \n",
    "        normalized_data = normalize_data_empirical(full_data)\n",
    "        \n",
    "        results_simulated = zhao_2015a_cue_integration_model(normalized_data)\n",
    "        results_simulated['full data']['VPCode'] = VP + ' '  + experiment\n",
    "        results_simulated['full data']['experiment'] = experiment\n",
    "        \n",
    "        VP_cue_integration_simulated[VP + ' '  + experiment] = results_simulated\n",
    "        VPs_simulated.append(VP + ' '  + experiment)\n",
    "\n",
    "    for VP in VPs_simulated:\n",
    "        full_results = evaluate_zhao_cue_integration(results = VP_cue_integration_simulated[VP], kind = 'Model')\n",
    "        full_results['VPCode'] = VP\n",
    "        full_results['experiment'] = experiment\n",
    "        cue_integration_results.append(full_results)\n",
    "\n",
    "cue_integration_results = pd.concat(cue_integration_results)\n",
    "cue_integration_results['SD'] = cue_integration_results['circ_sd'] \n",
    "cue_integration_results['VPCode'] = cue_integration_results['VPCode'] + '_' +  cue_integration_results['kind']\n",
    "\n",
    "\n",
    "#both experiments\n",
    "plt.figure(figsize = (3.0,2))    \n",
    "p = sns.barplot(x = 'experiment', y = 'circ_sd', hue = 'condition', data = cue_integration_results, palette=colours)\n",
    "plt.ylim(0,55)\n",
    "plt.ylabel('Response Variability \\n (Mean Circular SD,°)')\n",
    "plt.suptitle('Zhao 2015 \\n Model Simulations ', y = 1.15)\n",
    "sns.despine()\n",
    "p.legend_.remove()\n",
    "plt.show()\n",
    "\n",
    "# _____________________________________ Cue Integration Analysis (Linear) ______________________________________ #\n",
    "\n",
    "simulated = pd.concat([VP_cue_integration_simulated[VP]['full data'] for VP in VP_cue_integration_simulated.keys()])\n",
    "simulated['kind'] = 'Model'\n",
    "simulated['theta_deg'] = -simulated['theta_deg']\n",
    "\n",
    "all_data = pd.concat([simulated])\n",
    "all_data = all_data[all_data.condition == 'conflict']\n",
    "all_data = all_data[all_data.landmark_rotation <= 90.0]\n",
    "\n",
    "g = sns.catplot(x=\"landmark_rotation\", y=\"theta_deg\", hue=\"kind\", col=\"experiment\",\n",
    "                capsize=.2, palette=\"YlGnBu_d\", height=2.5, aspect=.85,\n",
    "                kind=\"point\", data=all_data)\n",
    "\n",
    "\n",
    "g.despine(left=True)\n",
    "g.set_titles(row_template = '{row_name}', col_template = '{col_name}')\n",
    "g.set_axis_labels('')\n",
    "g.axes[0,0].set_ylabel('Homing Direction (°)')\n",
    "g.figure.suptitle('Response Bias (Heading)', y = 1.05)\n",
    "g.figure.supxlabel('Landmark Shift (°)')\n",
    "\n",
    "g.set(ylim = (0,110), yticks = [15,30,45,90])\n",
    "\n",
    "\n",
    "\n",
    "g._legend.remove()\n",
    "#plt.savefig(path + 'zhao_2015_vs_model_simulation_heading_direction.svg')\n",
    "\n",
    "g = sns.catplot(x=\"landmark_rotation\", y=\"circ_sd\", hue=\"kind\", col=\"experiment\",\n",
    "                capsize=.2, palette=\"YlGnBu_d\", height=2.5, aspect=.85,\n",
    "                kind=\"point\", data=all_data)\n",
    "\n",
    "g.despine(left=True)\n",
    "g.set_titles(row_template = '{row_name}', col_template = '{col_name}')\n",
    "g.set_axis_labels('')\n",
    "g.axes[0,0].set_ylabel('Heading Variability \\n (Mean Circular SD, °)')\n",
    "g.figure.suptitle('Response Variability (Heading)', y = 1.05)\n",
    "g.figure.supxlabel('Landmark Shift (°)')\n",
    "g._legend.remove()\n",
    "\n",
    "\n",
    "g.set(ylim = (0,30), yticks = [0,10,20,30])\n",
    "\n",
    "plt.legend(frameon = False, ncol = 2, bbox_to_anchor = (-1,-0.185,0,0), title = '       kind')\n",
    "\n",
    "plt.savefig(path + 'zhao_2015_vs_model_simulation_heading_variability.svg', transparent = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
